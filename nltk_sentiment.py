#This will be our first shot at using natural language to analyze sentiment
import nltk
import random
from nltk.classify.scikitlearn import SklearnClassifier
import pickle
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import SVC, LinearSVC, NuSVC
from nltk.classify import ClassifierI
from statistics import mode
from nltk.tokenize import word_tokenize, sent_tokenize
import re 
from nltk.corpus import stopwords
from nltk.text import Text 
import string 
from nltk.stem import PorterStemmer
from nltk import RegexpParser
import xlrd 

from nltk.corpus import conll2000
from nltk_helpers import *
import pickle 
from nltk.chunk.util import tree2conlltags, conlltags2tree



nltk.download('averaged_perceptron_tagger')
ps = PorterStemmer()
stop_words = list(set(stopwords.words('english')))







#Note this function takes in a string like object, it is therefore best
#to run this before any other preprocessing takes place, HOWEVER doing this 
#will prevent things like converting to sentences, etc. from running properly
def remove_punctuation(sentence):
    sentence = re.sub(r'[^\w\s]', '', sentence)
    return sentence 

def convert_doc_to_sentences(doc):
    return sent_tokenize(doc)



def clean_list_of_sentences(sentence_list):
    return [remove_punctuation(sentence) for sentence in sentence_list]

#This function can be used on either an "un-cleaned" or a "cleaned" sentence list
#Note that this function returns a list of lists - to get a single, unnested word list
#we should use the "nested list to list" function
def convert_sentence_list_to_word_list(sentence_list):
    return [word_tokenize(sentence) for sentence in sentence_list]

#This function will generally be used on a list generated by the preceding function,
#"convert sentence list to word list" to give us an unnested list of word for further 
#processing at the word level
def nested_list_to_list(list1):
    unnested = []
    for sublist in list1:
        for item in sublist:
            unnested.append(item)
    return unnested

#This function takes in either a sentence, or a single *long* list of words, 
#possibly after converting a document to sentences, sentences to words and then
#using the unnesting function - then running this will return a *long* list modulo 
#stop_words
def remove_stopwards(sentence):
    return [w for w in sentence if not w in stop_words]

#This function is a "modified" version of the remove_punctuation function. Note that this
#function operates on a list - preferably a *long* list of words. It will return a list
#but with non-alphanumeric, non-whitespace characters removed
def remove_punctuation_from_list(l):
    expression = re.compile(r'[^\w\s]')
    return [word for word in l if not expression.match(word)]

#this takes in a list (NOT A LIST OF LISTS) and returns the list with each element 
#tagged per its part of speech - note this makes sense only for a list of words 
def part_of_speech_tagging(word_list):
    return nltk.pos_tag(word_list)

#This function takes in a document (as a string) and outputs a list of words,
#which have been "cleaned" of stopwords and punctuation, this allows for
#easy part of speech tagging / stemming / further processes on the "word-list"
def clean_document(doc):
    sentences = convert_doc_to_sentences(doc)
    words = convert_sentence_list_to_word_list(sentences)
    unnested_word_list = nested_list_to_list(words)
    unnested_word_list = remove_stopwards(unnested_word_list)
    unnested_word_list = remove_punctuation_from_list(unnested_word_list)
    return unnested_word_list


#This function takes in a list of words, and "stems" them
def stem_word_list(list1):
    return [ps.stem(w) for w in list1]


def npchunk_features(sentence, i, history):
     word, pos = sentence[i]
     #if (i == 0):
     #    print(word)
     #    print(pos)
     #    print("DONE")
     #    exit(1)
     return {"pos": pos}

class ConsecutiveNPChunkTagger(nltk.TaggerI):
    #Note this class requires training_sentences - a list of lists, 
    # each sublist consists of a tuple for each word in the sentence
    #wand the tuple consists of (word, POS) pairs. I.E. we take in
    #[...[(word_1, POS1_), ...(word_N, POS_N)]]
    def __init__(self,training_sentences):
        train_set = []
        for tagged_sentence in training_sentences:
            untagged_sentence = nltk.tag.untag(tagged_sentence)
            history = []
            for i, (word, tag) in enumerate(tagged_sentence):
                featureset = npchunk_features(untagged_sentence, i, history)
                train_set.append((featureset, tag))
                history.append(tag)
        self.classifier = nltk.NaiveBayesClassifier.train(train_set)
        
    
    def tag(self, sentence):
        history = []
        for i, word in enumerate(sentence):
            featureset = npchunk_features(sentence, i, history)
            tag = self.classifier.classify(featureset)
            history.append(tag)
        return zip(sentence, history)


class ConsecutiveNPChunker(nltk.ChunkParserI):
    def __init__(self, training_sentences):
        tagged_sentences = [[((w, t), c) for (w, t, c) in 
        nltk.chunk.tree2conlltags(sentence)] for sentence in training_sentences]
        self.tagger = ConsecutiveNPChunkTagger(tagged_sentences)

    def parse(self, sentence):
        tagged_sents = self.tagger.tag(sentence)
        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]
        return nltk.chunk.conlltags2tree(conlltags)

#This next section contains functions and items specifically relating to our test excel sheet
#///////////////////////////////////////////////////////////
simple_grammar = r"""
    NP: {<DT>*<JJ>*<CD>?<NN.*>*<IN>+<NN.*>+}
        {<DT | PP | PRP | PRP\$>?<JJ.*>*<CD>?<NN.*>+<POS>?<CD>?<NN.*>+}
        {<\$><CD>}
        

"""
train_sentences = conll2000.chunked_sents('train.txt', chunk_types = ['NP'])
chunker_1 = ConsecutiveNPChunker(train_sentences)
filename = 'chunker_1_pickle'
#outfile = open(filename, 'wb')
#pickle.dump(chunker_1, outfile)
#outfile.close()

#infile = open(filename, 'rb')
#newcl1 = pickle.load(infile)
#infile.close()


data = conll2000.chunked_sents()
length = len(data)
train_test_division = length - int(length / 10)
train_data = data[:train_test_division]
test_data = data[train_test_division:]
t = nltk.chunk.util.tree2conlltags(train_data[0])


#This function takes in a list of trees, the Kth tree being the inverse of the
#IOB method for the Kth sentence, it returns a list of lists, with the Kth sublist
#consisting of the POS_tags and IOB_tags for the words in the Kth sentence
def conll_tag_chunks(chunk_sents):
    #tagged_sents is a list of lists, with the Kth sublist consisting of tuples of the form
    #(word, POS_tag, IOB_tag) for each word in the Kth sentence
    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]
    return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]
    









test_sentences = conll2000.chunked_sents('test.txt', chunk_types = ['NP'])
#print(chunker.evaluate(test_sentences))
#print("hello")
#bigram_chunker = BigramChunker(train_sentences)
#print(unigram_chunker.evaluate(test_sentences))

#postags = sorted(set(pos for sentence in train_sentences for (word, pos) in sentence.leaves()))
#print(unigram_chunker.tagger.tag(postags))






#////////////////////////////////////////////////////


